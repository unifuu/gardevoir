---
title: LangChainについて
date: 2026-01-02
summary: 📝 LangChainのメモ...
tags:
  - ai
  - agent
  - llm
  - langchain
---

LangChain は、LLM（大規模言語モデル）を利用したアプリケーション開発を簡素化するために設計された、オーケストレーションフレームワークです。

その核心において、生の LLM は単なるテキスト処理エンジンに過ぎません。テキストを入力すれば、テキストが返ってくるだけです。しかし、実際のアプリケーションを構築するには、そのエンジンを外界――データベース、ユーザーのコンテキスト（文脈）、API ツール、会話履歴――と接続する必要があります。LangChain は、これらの接続を堅牢かつ再利用可能にするための「接着剤」を提供します。

---

## 1. コア哲学：「チェーン」と構成可能性 (Composition)

LangChain が解決する根本的な問題は、**コンポーザビリティ（構成可能性）**です。

標準的なスクリプトでは、OpenAI にプロンプトを手動で送信し、返ってきた文字列を受け取り、正規表現（RegEx）で解析し、それをデータベースに送信するといった処理を書くことになります。LangChain は、これらのステップを再利用可能なコンポーネントとして標準化し、互いに「チェーン（連鎖）」できるようにします。

### LCEL (LangChain Expression Language) への移行

現代の LangChain では、**LCEL**と呼ばれる宣言的な構文を使用します。他の関数を呼び出す Python 関数を書く代わりに、Unix のパイプ演算子（`|`）を使用してワークフローを定義します。

- **旧来の方法:** 「変数 A を取り、関数 B に渡し、結果を C に保存し、C を関数 D に渡す」
- **LCEL の方法:** `A | B | D`

これにより、パイプラインの可読性が高まり、修正も容易になります。このパイプライン内のすべてのオブジェクトは、**Runnable Interface**（実行可能インターフェース）と呼ばれる標準プロトコルに従っています。つまり、どのコンポーネントも同じように振る舞います（すべてが `.invoke()`、`.stream()`、`.batch()` といったメソッドを持っています）。

---

## 2. アトミックな構成要素 (Atomic Building Blocks)

複雑なアーキテクチャを理解する前に、LangChain アプリの 90%を構成する 3 つの特定のプリミティブ（基本要素）を理解する必要があります。

### A. モデル (エンジン)

LangChain は 2 種類のモデルを区別していますが、多くの場合、基礎となる技術は同じです。

1.  **LLM:** 純粋なテキスト補完。入力は文字列、出力も文字列。（レガシー）
2.  **Chat Models（チャットモデル）:** 構造化された会話。入力はメッセージのリスト（System, Human, AI）、出力は AI メッセージ。（現在の標準）

**なぜこれが重要か:** チャットモデルを使用すると、「指示」（System Message）と「ユーザーのクエリ」（Human Message）を分離できるため、プロンプトインジェクションのリスクが大幅に低減し、指示の遵守率が向上します。

### B. プロンプト (指示書)

コード内に文字列をハードコーディングするのは脆（もろ）いアプローチです。**Prompt Templates（プロンプトテンプレート）**を使用すると、プロンプトをコードのように扱うことができます。これらは、プロンプトの「構造」とその中の「変数」を分離します。

- _概念:_ プロンプトテンプレートは、「Python の f-string の超強化版」と考えてください。ロジック、役割の区別（System 対 User）、フォーマットを自動的に処理します。

### C. 出力パーサー (翻訳者)

LLM は非構造化テキストを返します。ソフトウェアアプリケーションを構築する場合、次のコードステップに渡すために、通常は構造化データ（JSON、リスト、日付など）が必要です。

- **Output Parsers（出力パーサー）**は、LLM からの生のテキストを受け取り、特定の Python オブジェクトに変換します。

---

## 3. 検索拡張生成 (RAG)

RAG は、LangChain の最も一般的なユースケースです。これは、「どうすれば LLM に私のプライベートなデータを知らせることができるか？」という問題を解決します。

このプロセスは、2 つのフェーズで実行されるパイプラインです。

### フェーズ 1：インデックス作成 (セットアップ)

1.  **Load (ロード):** ソース（PDF、SQL、Web サイト）からデータを取り込みます。
2.  **Split (分割):** LLM が一度に読めるテキスト量（コンテキストウィンドウ）には制限があります。大きなドキュメントを小さな「チャンク」に分割する必要があります。
3.  **Embed (埋め込み):** エンベディングモデルを使用して、それらのチャンクを**ベクトル**（数値のリスト）に変換します。これはテキストの「意味」を数学的に表現したものです。
4.  **Store (保存):** これらのベクトルを**ベクトルデータベース**に保存します。

### フェーズ 2：検索 / リトリーバル (実行時)

ユーザーが質問をした際の流れは以下の通りです。

1.  LangChain はユーザーの質問をベクトルに変換します。
2.  データベースをスキャンし、質問と数学的に類似している（関連性の高い）チャンクを探します。
3.  それらのテキストチャンクを取得（リトリーブ）します。
4.  それらのチャンクをシステムプロンプトのコンテキストに貼り付けます。
5.  結合されたプロンプトを LLM に送信し、回答を生成させます。

---

## 4. エージェント：「チェーン」から「推論」へ

もし**Chain（チェーン）**が線路（列車は A から B、そして C へと決まった道を進む）だとすれば、**Agent（エージェント）**はタクシー運転手です。

チェーンでは、ステップの順序はあなたによってハードコーディングされています。一方、エージェントでは、**LLM が推論エンジンとして機能します**。

1.  エージェントに一連の**ツール**（例：「Google 検索」、「電卓」、「天気予報 API」）を与えます。
2.  ユーザーが質問をします。
3.  エージェントは判断します。「答えを知っているか？いや、知らない。『Google 検索』ツールを使う必要があるな」
4.  エージェントは検索入力を生成し、ツールを実行し、出力を観察し、_その上で_ 次に何をすべきかを決定します。

これにより、**思考 (Thought) -> 行動 (Action) -> 観察 (Observation) -> 思考 (Thought)** という動的なループが生まれます。

---

## 5. 最小コード例 (LCEL)

以下は、「アトミックなブロック」（プロンプト、モデル、パーサー）が LCEL のパイプ構文（`|`）を使ってどのように接続されるかを視覚化するための必須の例です。

この例は、ユーザーの入力をジョークの構造に変換するものです。

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. モデル (The Model)
model = ChatOpenAI(model="gpt-4")

# 2. プロンプト (動的な指示)
prompt = ChatPromptTemplate.from_template(
    "{topic} についての短いジョークを教えて。"
)

# 3. 出力パーサー (出力を整形・整理する)
parser = StrOutputParser()

# 4. チェーン (パイプでこれらを接続)
chain = prompt | model | parser

# 5. 実行
# 辞書入力がプロンプト内の {topic} 変数を埋めます
result = chain.invoke({"topic": "アイスクリーム"})

print(result)
# 出力例: "アイスクリームのトラックが故障した理由は？ 道がロッキーロード（岩だらけの道／アイスのフレーバー）だったから！"
```
